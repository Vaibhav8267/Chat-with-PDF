{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5ab631",
   "metadata": {},
   "source": [
    "# 1. Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e27bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../SRC')\n",
    "\n",
    "from pdf_utils import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00b6c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rohitrawat/github-repos/Chat-with-PDF/Notebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec110a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
      "\n",
      "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_file = \"../data/248_Paper.pdf\"\n",
    "text = extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272943f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenMIC-2018: AN OPEN DATASET FOR MULTIPLE INSTRUMENT\\nRECOGNITION\\nEric J. Humphrey\\nSpotify\\nejhumprey@spotify.com\\nSimon Durand\\nSpotify\\ndurand@spotify.com\\nBrian McFee\\nNew York University\\nbrian.mcfee@nyu.edu\\nABSTRACT\\nIdentiﬁcation of instruments in polyphonic recordings is a\\nchallenging, but fundamental problem in music informa-\\ntion retrieval. While there has been signiﬁcant progress in\\ndeveloping predictive models for this and related classiﬁ-\\ncation tasks, we as a community lack a common data-set\\nwhich is large, freely available, diverse, and representative\\nof naturally occurring recordings. This limits our ability to\\nmeasure the efﬁcacy of computational models.\\nThis article describes the construction of a new, open\\ndata-set for multi-instrument recognition. The dataset con-\\ntains 20,000 examples of Creative Commons-licensed mu-\\nsic available on the Free Music Archive. Each example is a\\n10-second excerpt which has been partially labeled for the\\npresence or absence of 20 instrument classes by annotators\\non a crowd-sourcing platform. We describe in detail how\\nthe instrument taxonomy was constructed, how the data-\\nset was sampled and annotated, and compare its character-\\nistics to similar, previous data-sets. Finally, we present ex-\\nperimental results and baseline model performance to mo-\\ntivate future work.\\n1. INTRODUCTION\\nMusic information retrieval (MIR) applications often de-\\npend on statistical models and machine learning algorithms\\nto relate audio content to semantically meaningful repre-\\nsentations. The development and evaluation of these meth-\\nods, in turn, depends on access to data, typically audio\\nrecordings which have been annotated for a particular task\\nsuch as chord recognition or tag prediction. Ideally, the\\ndata we use to develop and evaluate models should be\\nlarge, diverse, and open access, so that we as researchers\\nand engineers can diagnose failure modes and propose im-\\nprovements. However, because the vast majority of music\\nis subject to copyright, this has historically been difﬁcult\\nto achieve. This has resulted in a proliferation of de facto\\nstandard data-sets which are small, biased, and not freely\\navailable, which ultimately impedes scientiﬁc progress.\\nc⃝Eric J. Humphrey, Simon Durand, Brian McFee. Li-\\ncensed under a Creative Commons Attribution 4.0 International License\\n(CC BY 4.0). Attribution:\\nEric J. Humphrey, Simon Durand, Brian\\nMcFee. “OpenMIC-2018: An open dataset for multiple instrument recog-\\nnition”, 19th International Society for Music Information Retrieval Con-\\nference, Paris, France, 2018.\\nTo address this problem, McFee et al. [15] proposed\\nan iterative evaluation framework for developing open ac-\\ncess data-sets for MIR, with a speciﬁc focus on instrument\\nrecognition. While this proposal was apparently met with\\nenthusiasm from the community, little progress has been\\nmade in the intervening time toward enacting the proposal.\\nWe hypothesize that this was primarily due to two factors:\\na lack of a conveniently accessible audio data, and the ex-\\npense of creating the initial development set. Recently, two\\ncomplementary data-sets have been published, which we\\ncombine here to resolve both of these issues: the Free Mu-\\nsic Archive data-set [8], and AudioSet [11]. The result is a\\ndiverse, open access collection of 20,000 audio clips anno-\\ntated for the presence of 20 distinct instrument categories,\\nwhich we denote as OpenMIC-2018.\\n1.1 Our contributions\\nOur primary technical contribution is a new, open dataset\\nfor training and evaluating instrument recognition algo-\\nrithms. This article describes in detail how the dataset was\\nconstructed by using a combination of model transfer from\\nprevious datasets and crowd-sourced annotation. Our goals\\nin documenting the data construction process are two-fold.\\nFirst, it provides transparency around the various decisions\\nand compromises made in this speciﬁc dataset. Second, we\\ndescribe technical issues and general solutions which may\\nbe of interest to future developers of music datasets.\\n1.2 Related work\\nInstrument recognition, either monophonic or polyphonic,\\nis a long-standing problem in MIR, and many datasets for\\nevaluating methods have been developed over the years.\\nTable 1 lists some of the commonly used datasets, along\\nwith various descriptive attributes.\\nOf speciﬁc interest\\nare the size of the collections, the number of instrument\\nclasses, the duration of each example, the diversity of the\\ncollection (e.g., genre or style), whether the examples are\\npolyphonic, the number of instrument labels per example,\\nand whether the data is open access.\\nBroadly speaking, existing datasets can be broken into\\ntwo categories, according to whether samples contain\\nnotes played by isolated instruments (RWC [12], Good-\\nsounds [1], or NSynth [10]), or recordings of instrument\\nensembles. Datasets of isolated instrument recordings are\\noften easier to produce and annotate at large scale because\\nlong recordings spanning multiple notes can be segmented\\n438\\nTable 1. A qualitative comparison of different existing datasets for instrument identiﬁcation.\\nCollection\\n# Examples\\n# Instruments\\nDuration\\nDiverse\\nPolyphonic\\nMulti-label\\nOpen\\nRWC\\n[12]\\n3,544\\n50\\nscale\\nGood-sounds [1]\\n6,548\\n12\\nnote\\n✓\\nNSynth\\n[10]\\n305,979\\n1,006\\nnote\\n✓\\nMedleyDB\\n[3]\\n122\\n80\\nsong\\n✓\\n✓\\n✓\\nMusicNet\\n[19]\\n330\\n11\\nsong\\n✓\\n✓\\n✓\\nIRMAS\\n[4]\\n6,705\\n11\\n3s\\n✓\\n✓\\nOpenMIC-2018\\n20,000\\n20\\n10s\\n✓\\n✓\\n✓\\n✓\\nto generate examples with a shared label. However, the\\nacoustic properties of ensemble recordings differ signiﬁ-\\ncantly from those of isolated recordings, so models devel-\\noped on single-instrument data often do not generalize to\\nthe polyphonic case. Conversely, ensemble recordings are\\ntypically difﬁcult to precisely annotate, which results in\\neither high-quality collections with a small number of dis-\\ntinct tracks (MedleyDB [3] or MusicNet [19]), or in col-\\nlections with more tracks but with only partial annotations\\n(such as IRMAS [4] with predominant instrument tags for\\nshort excerpts). An ideal dataset would be large, diverse,\\nstrongly annotated (including both positive and negative\\nexamples), and freely available, so that at each instant in\\nany recording, full information about all active instruments\\nis available. While existing datasets succeed on some of\\nthese criteria, none achieves all simultaneously.\\n1.3 The Free Music Archive\\nThe Free Music Archive 1 (FMA) is a web-based repos-\\nitory of freely available music recordings.\\nRecently, a\\nsnapshot of FMA has been released to the research com-\\nmunity to facilitate content-based music analysis evalua-\\ntion [8]. The FMA snapshot includes 106,574 tracks by\\nsome 16,341 artists, along with pre-computed features.\\nEach track is annotated with both coarse (16 categories)\\nand ﬁne (161 categories) genre tags. Tracks are provided\\nunder a small variety of licenses, with the vast majority\\nbeing Creative Commons [7].\\nThis allows practitioners\\nto archive and redistribute data (with some minor restric-\\ntions), which is fundamental to the practice of open and\\nreproducible scientiﬁc research.\\nWhile previous authors have noted the particular genre\\nbiases present on FMA [8], it nonetheless provides a large\\npool of realistic musical content which could be used in re-\\nsearch applications. Despite the speciﬁc quirks of the FMA\\ncollection, using it as a basis for large-scale MIR evalua-\\ntion has several beneﬁts. In addition to the obvious beneﬁts\\nof being open access, it also facilitates data revision and in-\\nclusion of new contributions from the community at large.\\nThis in turn makes it easier for corrections to be integrated,\\nand the collection to grow over time and not become stale.\\n2. CONSTRUCTING OpenMIC-2018\\nIn developing OpenMIC-2018, we took inspiration from\\nImageNet [9].\\nImageNet was constructed by selecting\\n1 http://freemusicarchive.org/\\nInstrument model\\nCandidate construction\\nSelection / annotation\\nInstrumentDNN\\nInstrumentDNN\\nInstrumentDNN\\nInstrumentDNN\\nVGGish features\\nAudioSet\\nFMA\\nVGGish +\\nInstrumentDNN\\nClip likelihoods\\nClip likelihoods\\nQuantile sampling\\n \\nOpenMIC \\n2018\\n \\nOpenMIC \\n(audio)\\nCF\\nFigure 1.\\nA multi-label instrument detector (Instru-\\nmentDNN, section 2.2) is trained on AudioSet data. The\\nmodel is used to score each 10s clip in FMA by likeli-\\nhood of each instrument (section 2.3). Clips are sorted\\ninto quantiles for each instrument, then sub-sampled and\\nannotated by CrowdFlower workers (section 3).\\nand annotating natural images to represent categories (syn-\\nonym sets, or synsets) drawn from the WordNet ontol-\\nogy [16], with a goal of having at least 500 positive ex-\\namples for each category. Candidate images were selected\\nby querying image search engines for each category term,\\nand then labels were veriﬁed by crowd-sourced annotation.\\nThe label correction and veriﬁcation step was critical at\\nthe time, due to the poor accuracy of image search engines\\nwhen the dataset was constructed in 2009.\\nWe follow a similar strategy here, with a few notable\\nmodiﬁcations. Rather than querying the Internet for candi-\\ndate samples, we restrict attention to freely available con-\\ntent hosted on the Free Music Archive, and speciﬁcally\\nthose with explicit Creative Commons licensing. Addi-\\ntionally, instead of the WordNet ontology, we use the re-\\ncently published AudioSet concept ontology [11], which\\nitself derives from WordNet, but is adapted to acoustically\\nmeaningful concepts. Using existing AudioSet data, we\\nconstruct a multi-instrument estimator and use this model\\nto rank the unlabeled FMA data and provide candidates for\\nannotation. The remainder of this section describes the en-\\ntire process in detail, which is visualized in Figure 1.\\n2.1 AudioSet\\nAudioSet is a recently released concept ontology and\\nhuman-annotated dataset derived from YouTube videos,\\nwith the goal of providing a testbed for identifying acoustic\\nevents [11]. The ontology consists of 632 classes, repre-\\nsented as a lattice-like graph, rather than hierarchical tree\\nstructure, i.e. one low-level class may have two distinct\\nparents. The annotated dataset consists of at least 100 pos-\\nitive examples of 485 classes, distributed (non-uniformly)\\nacross nearly 1.8M video clips of 10 seconds (or less)\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\n439\\ndrawn from YouTube. Similar in spirit to the work pre-\\nsented here, AudioSet is motivated by a lack of large-scale\\nannotated audio data for scientiﬁc research purposes.\\nWhile the AudioSet ontology includes musical instru-\\nments, the audio data does not match our requirements for\\nan open music instrument sample. The collection is de-\\nrived from YouTube videos, for which there are no guar-\\nantees on the legality of licensing, sharing, and archiv-\\ning the content. Though abstract features are made avail-\\nable via a publicly available acoustic model, an inability to\\nmake the source content directly accessible has limited the\\nvalue of other large collections, such as the Million Song\\nDataset [2]. Furthermore, the content is often quite differ-\\nent from musical performances, an important characteristic\\nat the root of what makes this task both challenging and in-\\nteresting: many of the positively labeled examples are solo\\nperformances, which makes it difﬁcult to model and evalu-\\nate on realistic, highly correlated ensemble performances.\\nThat said, AudioSet serves two important functions in\\nthis project. It is impractical to annotate the entire FMA\\ncollection of more than 100K recordings outright; how-\\never, it is also extremely unlikely that one could draw a\\nrandom subsample with sufﬁcient representation across a\\nnumber of instruments. The occurrence of musical instru-\\nments is heavily biased by popularity, such as voice, gui-\\ntar, or piano, and this is especially true in the Free Mu-\\nsic Archive. Here, we leverage AudioSet to build a multi-\\ninstrument estimator that allows us to sub-sample and more\\nefﬁciently use annotation resources. For better or worse,\\nwe also leverage the previous work in ontology construc-\\ntion, while circumventing the important, but difﬁcult, chal-\\nlenge of selecting which instruments to consider: here, we\\nare limited to only those with enough signal in AudioSet\\non which to build a baseline model.\\nWe manually identify the classes that correspond to mu-\\nsical instruments, resulting in a set of more than 70 relevant\\nclasses. For the sake of coverage, they are merged into “in-\\nstruments”, e.g. “Acoustic Guitar”, “Electric Guitar”, and\\n“Tapping (guitar technique)” become guitar, while “Cello”\\nand “Violin” remain distinct. Note that this class resolu-\\ntion is intentionally approximate, as the long-term goals\\nof this project include iteratively reﬁning these concepts\\nas acoustic models improve. We then ﬁlter the 1.8M clips\\nin AudioSet to those containing these classes. Unsurpris-\\ningly, the distribution skews toward instruments common\\nin Western popular music, such as guitar, violin, or drums,\\nand we cut this list at 1500 examples. Additionally, we\\nrandomly draw 8000 non-musical examples as negative\\ninstances for building the instrument model described in\\nsection 2.2. In summary, the resulting instrument subset\\nconsists of 206K clips, totalling roughly 2M seconds (570\\nhours) of annotated content for 23 instruments. 2\\n2.2 Multi-instrument modeling\\nAudioSet offers no licensing guarantees on the source con-\\ntent, and there is no approved mechanism for directly ac-\\ncessing the audio data. To make the dataset more gener-\\n2 https://github.com/cosmir/open-mic-data\\nally useful, the developers of AudioSet have released both\\na pre-trained feature embedding model [13] based on the\\nVGG architecture for object detection in images [18], 3\\nand its outputs over the original AudioSet audio signals. 4\\nThis model, referred to as “VGGish”, produces a 128-\\ndimensional feature vector every 0.96 seconds with an\\nequal window size, such that adjacent features capture non-\\noverlapping context. VGGish features are ZCA-whitened\\nand each coefﬁcient is quantized to 8-bits to reduce the\\nfootprint of the dataset.\\nUsing the sub-sampling process described above, we ﬁl-\\ntered the AudioSet features down to those clips relevant\\nfor the instrument ontology considered here. The data are\\nconditionally partitioned by YouTube ID into training, val-\\nidation, and test splits with a 3 : 1 : 1 ratio. We randomly\\ngenerate over 200 unique, fully connected deep network ar-\\nchitectures and hyper-parameter conﬁgurations, spanning\\ndepth (1–8 layers), width (128 to 2048 units, by powers\\nof 2), the application of dropout and batch normalization,\\ndifferent optimization algorithms (stochastic gradient de-\\nscent, RMSProp, and Adam [14]), as well as various pa-\\nrameters for each operation. All models are trained for 50\\nepochs of the training data, and the parameter checkpoint\\nwith the highest macro-F1 (class-averaged) score over the\\nvalidation data is taken as the best model.\\nOverall, we ﬁnd that roughly 15% of the models be-\\nhave with statistical equivalence, achieving a mean macro-\\nF1 score of 0.514 (σ = 0.0095) and a micro-F1 (item-\\naveraged) score of 0.656 (σ = 0.0056) on the test partition.\\nThe best conﬁguration is determined to be a 7-layer net-\\nwork, with widths of [1024, 512, 256, 1024, 256, 1024, 23],\\nbatch-normalization on the ﬁrst four layers, and point-\\nwise dropout applied to the inputs of the last ﬁve\\n[0.0, 0.0, 0.25, 0.125, 0.25, 0.25, 0.5]. The winning model,\\nwhich we refer to as InstrumentDNN, is trained with the\\nAdam optimizer in Keras for 8 epochs, with a learning rate\\nof 0.0001 and a β1 of 0.99. For reproducibility, the train-\\ning data and trained model are made publicly available in\\nthe source repository.\\n2.3 FMA clip sampling\\nThe VGGish model is applied to each track in FMA,\\nand the resulting ZCA features are processed by Instru-\\nmentDNN to produce time-varying instrument likelihoods.\\nFull tracks are then divided into candidate clips by per-\\nforming maximum-likelihood aggregation over 10 second\\nwindows with a 4 second hop size. To account for fram-\\ning effects, the maximum likelihood of each instrument is\\ntaken over the middle 8 seconds, centered on the frame.\\nThis produces over 7M clip candidates.\\nWe ultimately want an approximately balanced sample\\nthat has good positive representation of each instrument\\nclass. Therefore the candidate set is sub-sampled by the\\nfollowing process.\\nFirst, we consider the median like-\\n3 https://github.com/tensorflow/models/tree/\\nmaster/research/audioset\\n4 https://research.google.com/audioset/\\ndownload.html\\n440\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\nlihood of each class over all candidates, and sort instru-\\nments in ascending order, as a proxy for class occurrence\\nin the FMA. Then, proceeding from least to most likely\\ninstrument class, the clip candidate set is reordered by de-\\nscending conditional class likelihood. We randomly se-\\nlected N instances from the 99th percentile rank of that\\nclass, such that no two clips share a source track, i.e. sam-\\npled clips are recording-independent. All remaining clip\\ncandidates that also share a common recording with any\\nsampled are discarded, and the process is repeated for the\\nnext instrument. For K instruments, the sampling process\\nyields N × K clips from distinct tracks. Initially, we set\\nN = 1000, K = 23, but manual inspection of the results\\nrevealed three classes that either InstrumentDNN cannot\\nreliably detect, are poorly represented in FMA, or both:\\nharp, bagpipes, and harmonica. We removed these classes,\\nleaving K = 20 instruments and 20K clips.\\n3. CROWD-SOURCED ANNOTATION\\nAt this stage, roughly 25M seconds of audio have been\\nsub-sampled to 200K, a 105 reduction, while rebalancing\\nfor instrument occurrence. Strongly labeling a collection\\nof this size is still cost-prohibitive, and we must be prag-\\nmatic with our annotation efforts. In tackling this chal-\\nlenge, one can think of annotation as a sparse, binary ma-\\ntrix completion problem where most of the values in the\\ninstrument occurrence matrix will be zero. Therefore an-\\nnotation effort is best allocated by ﬂattening this matrix\\ninto clip-instrument pairs, and prioritizing likely positives.\\nFramed this way, our most likely positives are iden-\\ntiﬁed by the clip selection process: each instrument has\\n1K potential positive examples that must be validated by\\nhuman annotators. We would also like to obtain a num-\\nber of strong negatives as well, and draw 500 instances\\nper class that fall in the bottom 10th likelihood percentile\\nfrom the space of examples contained in OpenMIC-2018.\\nInstrument-wise percentile thresholds are computed over\\nthe full space of clip candidates. In contrast to positive\\nsampling, negative samples are drawn working from most\\nto least likely instruments. This is because the most likely\\ninstrument categories will have the fewest potential strong\\nnegatives. Additionally, random sampling is constrained\\nto draw no more than three strong negatives per clip, so as\\nto distribute this information across the collection. Finally,\\nto capture potential correlations and confusions, all addi-\\ntional likelihoods in the 99th percentile rank of their re-\\nspective instrument classes are added to the pool of binary\\nquestions for human annotators. This results in 33,250 po-\\ntential positive and 10,000 potential negative binary esti-\\nmates for human validation, which makes up roughly 10%\\nof all possible clip-instrument judgements.\\nHaving identiﬁed the questions worth asking, audio an-\\nnotation presents unique design challenges around how to\\nbest ask these questions of humans. Unlike images, audio\\nclips cannot be scanned in parallel by humans, and must\\nbe auditioned sequentially. This encourages annotation de-\\nsigns that ask several binary questions about the same ex-\\nample. Our ﬁrst attempt to annotate OpenMIC-2018 took\\nFigure 2. An example annotation task, showing the Mel-\\nspectrogram visualization, playback, response ﬁeld, and li-\\ncensing meta-data.\\nthis approach, but we found that annotators struggled with\\nthe increased burden of simultaneously judging multiple\\ninstrument tags. This resulted in poor agreement, unhappy\\nannotators, and an increased level of effort and skill to\\ncomplete. Our second attempt used 20 separate annota-\\ntion tasks, one per instrument, and annotators were asked\\nto determine the presence or absence of a speciﬁc instru-\\nment across multiple recordings.\\nAnnotation was performed on the CrowdFlower 5 plat-\\nform (CF). In contrast to Amazon Mechanical Turk, CF\\nprovides quality controls on sets of questions, collectively\\ncalled a “job”. A single contributor can provide at most 50\\nresponses (or 10% of the job, whichever is larger), and a\\nquestion is ﬁnalized when annotators reach a set agreement\\nlevel and number of responses.\\nAdditionally, CF makes it easy to include control ques-\\ntions for which an answer is already known. These are\\nused to “quiz” contributors before they can perform any\\n(paid) work on a job, and remove contributors whose accu-\\nracy drops below a threshold, e.g. 70%. It is important that\\ncontrol questions use clear, unambiguous examples. While\\nthese can be easily identiﬁed for popular classes, it is difﬁ-\\ncult in the rare classes, notably mandolin and clarinet. For\\nthese classes, control questions were generated by rank-\\ning clips according to the margin between the target instru-\\nment’s likelihood and the maximum over other instruments\\nfor that clip, which gives preference toward clips where the\\ntarget instrument was both present and prominent.\\nEach question is a single judgement of an instrument’s\\npresence or absence for a given audio clip. As shown in\\nFigure 2, we use a radio button interface for the judge-\\nment, provide audio playback in the browser, and addition-\\nally display an approximately aligned Mel-spectrogram to\\nfacilitate the task, inspired by previous audio annotation\\nresearch [5]. Finally, we are legally obligated to display\\ntrack title, artist, and license information, which may pro-\\nvide coincidental information about a given track.\\n4. OpenMIC-2018 ANALYSIS\\nWe collected over 230K judgements from more than 2,500\\nunique contributors across the 20 instrument classes. Fig-\\nure 3 summarizes the resulting annotation distributions for\\n5 http://crowdflower.com\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\n441\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\n2000\\n# Clips\\naccordion\\nbanjo\\nbass\\ncello\\nclarinet\\ncymbals\\ndrums\\nflute\\nguitar\\nmallet_percussion\\nmandolin\\norgan\\npiano\\nsaxophone\\nsynthesizer\\ntrombone\\ntrumpet\\nukulele\\nviolin\\nvoice\\nPresent\\nAbsent\\nFigure 3. Statistics of crowd-sourced annotation for each\\ninstrument in OpenMIC-2018.\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nAnnotator agreement\\naccordion\\nbanjo\\nbass\\ncello\\nclarinet\\ncymbals\\ndrums\\nflute\\nguitar\\nmallet_percussion\\nmandolin\\norgan\\npiano\\nsaxophone\\nsynthesizer\\ntrombone\\ntrumpet\\nukulele\\nviolin\\nvoice\\nPresent\\nAbsent\\nFigure 4. Annotator agreement for each instrument.\\neach instrument. For each instrument class, the number\\nof conﬁrmed positive and negative clips are plotted sep-\\narately. Each class has at least 500 conﬁrmed positives,\\nand at least 1500 conﬁrmed positive or negative. Although\\nnot every clip is tagged for every instrument, the abun-\\ndance of strong negative labels facilitates supervised learn-\\ning and strong evaluation. Figure 4 summarizes the inter-\\nannotator agreements for each instrument’s presence or ab-\\nsence. Some instruments produce more agreement for ab-\\nsence than presence (accordion, violin), while the reverse\\nis true for others (synthesizer). Overall, we observed a high\\namount of agreement across all instruments.\\nFigure 5 compares InstrumentDNN’s predicted likeli-\\nhoods to the annotations for three instrument classes. In-\\nstrumentDNN produces a wide range of likelihood values\\non mandolin (ﬁg. 5, left), indicating that the 99th percentile\\nlikelihood is well below the threshold for positive detec-\\ntion. This is likely due to a combination of model calibra-\\ntion errors and poor representation in AudioSet. However,\\nthe sampling strategy still produced a large number of val-\\nidated positive examples. For more common classes, such\\nas cymbals (ﬁg. 5, center), there is a clearer distinction be-\\ntween the positive and negative selections. For the most\\ncommon classes, such as voice (ﬁg. 5, right), the vast ma-\\njority of positive selections are validated by the annotators\\nas positive, and conversely for the negative selections.\\nTo measure the diversity of the annotated subset, ﬁg. 6\\ncompares the distribution of genres over both the sample\\nand the background population of FMA. While both distri-\\nbutions exhibit non-uniform genre distributions, the sam-\\nple is fairly representative of FMA. The instrument-based\\nsampling does introduce some systematic bias, increasing\\nrepresentation of styles with distinctive instrumentation,\\nsuch as classical or jazz. This effect can be observed di-\\nrectly in ﬁg. 7, which shows the number of clips in each\\ngenre that are positively labeled for each instrument. For\\nexample, the majority of organ and piano examples are\\ntagged as classical, while synthesizer is drawn primarily\\nfrom electronic and experimental.\\n4.1 Experiment: baseline modeling\\nTo estimate the expected performance of standard methods\\non OpenMIC-2018, we conducted a set of baseline exper-\\niments. We trained independent binary classiﬁers for each\\ninstrument. We report the accuracy of each of those mod-\\nels on 100 splits of the data, randomly selecting 500 test\\ninstances, and splitting the resulting training set in 3 folds\\nfor hyper-parameter selection. As input representation, we\\nuse the mean and standard deviation of VGGish features\\nover the clip’s duration.\\nWe tested several baseline models, and for simplicity\\nreport only the best performing one: a random forest (RF)\\nclassiﬁer. The hyper-parameter search is done on the num-\\nber of trees ({10, 100, 1000}) and on the maximum depth\\nof the tree ({2, 4, 8}). We also report the bias point of\\neach instrument category, and the performance of Instru-\\nmentDNN. This last comparison point gives us a measure\\nof how much information is gained by the crowd-sourced\\nlabels. This experiment is done with the scikit-learn [17],\\nand the code to reproduce will be made available.\\nThe results are shown in ﬁg. 8. We see an overall gain\\nin accuracy of more than 10 percent point (pp) compared\\nto both the bias points and InstrumentDNN. The perfor-\\nmance difference can partly be explained by the difference\\nin training distributions between RF and InstrumentDNN,\\nand because a strong signal can be learned from the dataset.\\nThe RF model performance is also more consistent across\\ninstruments with only a 20 pp difference between the worst\\nand best instrument accuracy, compared to a 34 pp dif-\\nference for InstrumentDNN. The gain compared to In-\\nstrumentDNN is therefore larger on the more difﬁcult in-\\nstruments, such as saxophone, mandolin and ukulele. In\\nthat case the crowd-sourced judgments might provide more\\nvalue and help build a robust system.\\n5. CONCLUSION\\nOpenMIC-2018 should prove to be useful for developing\\nand evaluating instrument detection models. We note that\\nthe dataset is not “complete” in that not every clip has been\\nannotated for the presence or absence of every instrument.\\nWhile this is true for every instrument dataset—if one con-\\nsiders instruments outside its vocabulary—it is usually not\\n442\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAudioSet likelihood\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nCF annotations\\nmandolin\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAudioSet likelihood\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nCF annotations\\ncymbals\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAudioSet likelihood\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nCF annotations\\nvoice\\nFigure 5. The distribution of the initial model likelihood compared to the crowd-sourced annotations for three instruments.\\nEach dot represents a clip, the horizontal line indicates the majority vote threshold, and the marginal distributions of model\\nlikelihood and crowd agreement are shown as bar plots. Data have been randomly perturbed for clarity of visualization.\\n10\\n3\\n10\\n2\\n10\\n1\\nBlues\\nClassical\\nCountry\\nEasy Listening\\nElectronic\\nExperimental\\nFolk\\nHip-Hop\\nInstrumental\\nInternational\\nJazz\\nOld-Time / Historic\\nPop\\nRock\\nSoul-RnB\\nSpoken\\nFMA\\nSample\\nFigure 6. The distribution of (top) genres over the selected\\nsample clips, and the background population in FMA.\\ntaken into consideration as part of the dataset design.\\nMore generally, previous datasets have not typically\\nbeen designed with a plan for future correction, revi-\\nsion, and expansion.\\nWe are explicitly planning to ex-\\npand and revise the dataset over time, either by additional\\ncrowd-sourcing, semi-supervised learning [6], or incre-\\nmental evaluation [15]. OpenMIC-2018 will be placed un-\\nder version control, archived, and each revision will re-\\nceive a unique document object identiﬁer (DOI) via Zen-\\nodo. 6\\nIn addition to supporting corrections and expanded cov-\\nerage, we anticipate expanding the vocabulary beyond the\\ninitial 20 classes, both in breadth of instrument classes, and\\nin depth to provide reﬁnements of classes, such as alto sax-\\nophone and tenor saxophone rather than saxophone. Simi-\\nlarly, future work could re-use much of the framework de-\\nveloped here to annotate the same collection for a variety\\nof qualities beyond instrumentation, and facilitate the de-\\nvelopment of integrated multi-task models.\\nAcknowledgments.\\nB.M. is supported by the Moore-\\nSloan Data Science Environment at NYU.\\n6 http://about.zenodo.org/\\nBlues\\nClassical\\nCountry\\nEasy Listening\\nElectronic\\nExperimental\\nFolk\\nHip-Hop\\nInstrumental\\nInternational\\nJazz\\nOld-Time / Historic\\nPop\\nRock\\nSoul-RnB\\nSpoken\\naccordion\\nbanjo\\nbass\\ncello\\nclarinet\\ncymbals\\ndrums\\nflute\\nguitar\\nmallet_percussion\\nmandolin\\norgan\\npiano\\nsaxophone\\nsynthesizer\\ntrombone\\ntrumpet\\nukulele\\nviolin\\nvoice\\n0\\n40\\n80\\n120\\n160\\n200\\nFigure 7. The distribution of (top) genres over the positive\\ncandidate sets for each instrument.\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nAccuracy\\naccordion\\nbanjo\\nbass\\ncello\\nclarinet\\ncymbals\\ndrums\\nflute\\nguitar\\nmandolin\\nmallet_percussion\\norgan\\npiano\\nsaxophone\\nsynthesizer\\ntrombone\\ntrumpet\\nukulele\\nviolin\\nvoice\\nBaseline\\nBias\\nInstrumentDNN\\nFigure 8. Accuracy of the Random Forest baseline (red),\\nInstrumentDNN (blue), and the dataset bias (black). The\\nRandomForest was trained on OpenMIC-2018, while In-\\nstrumentDNN was trained on AudioSet.\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\n443\\n6. REFERENCES\\n[1] Giuseppe Bandiera, Oriol Romani Picas, Hiroshi\\nTokuda, Wataru Hariya, Koji Oishi, and Xavier Serra.\\nGood-sounds. org: A framework to explore goodness\\nin instrumental sounds. In Proceedings of the 17th\\nInternational Society for Music Information Retrieval\\nConference, pages 414–419, 2016.\\n[2] Thierry Bertin-Mahieux, Daniel P. W. Ellis, Brian\\nWhitman, and Paul Lamere. The Million Song Dataset.\\nIn Proceedings of the 12th International Society for\\nMusic Information Retrieval Conference, pages 591–\\n596, 2011.\\n[3] Rachel M Bittner, Justin Salamon, Mike Tierney,\\nMatthias Mauch, Chris Cannam, and Juan Pablo\\nBello. MedleyDB: A multitrack dataset for annotation-\\nintensive MIR research. In ISMIR, volume 14, pages\\n155–160, 2014.\\n[4] Juan J Bosch, Jordi Janer, Ferdinand Fuhrmann, and\\nPerfecto Herrera. A comparison of sound segregation\\ntechniques for predominant instrument recognition in\\nmusical audio signals. In ISMIR, pages 559–564, 2012.\\n[5] Mark Cartwright, Ayanna Seals, Justin Salamon, Alex\\nWilliams, Stefanie Mikloska, Duncan MacConnell,\\nE Law, J Bello, and O Nov. Seeing sound: Investigating\\nthe effects of visualizations and complexity on crowd-\\nsourced audio annotations. Proceedings of the ACM on\\nHuman-Computer Interaction, 1(1), 2017.\\n[6] Olivier Chapelle, Bernhard Schlkopf, and Alexander\\nZien. Semi-Supervised Learning. The MIT Press, 1st\\nedition, 2010.\\n[7] Creative\\nCommons.\\nAbout\\nthe\\nlicenses.\\n2015.\\nhttps://creativecommons.org/about/.\\n[8] Micha¨el\\nDefferrard,\\nKirell\\nBenzi,\\nPierre\\nVan-\\ndergheynst, and Xavier Bresson. FMA: A dataset\\nfor music analysis. In Proceedings of the 18th In-\\nternational Society for Music Information Retrieval\\nConference, ISMIR 2017, Suzhou, China, October\\n23-27, 2017, pages 316–323, 2017.\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\\ncal image database. In Computer Vision and Pattern\\nRecognition, 2009. CVPR 2009. IEEE Conference on,\\npages 248–255. IEEE, 2009.\\n[10] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander\\nDieleman, Douglas Eck, Karen Simonyan, and Mo-\\nhammad Norouzi. Neural audio synthesis of musical\\nnotes with wavenet autoencoders. 2017.\\n[11] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman,\\nAren Jansen, Wade Lawrence, R Channing Moore,\\nManoj Plakal, and Marvin Ritter. Audio set: An on-\\ntology and human-labeled dataset for audio events. In\\nAcoustics, Speech and Signal Processing (ICASSP),\\n2017 IEEE International Conference on, pages 776–\\n780. IEEE, 2017.\\n[12] Masataka\\nGoto,\\nHiroki\\nHashiguchi,\\nTakuichi\\nNishimura, and Ryuichi Oka. RWC music database:\\nMusic genre database and musical instrument sound\\ndatabase. 2003.\\n[13] Aren Jansen, Jort F Gemmeke, Daniel PW Ellis,\\nXiaofeng Liu, Wade Lawrence, and Dylan Freed-\\nman. Large-scale audio event discovery in one mil-\\nlion youtube videos. In Acoustics, Speech and Signal\\nProcessing (ICASSP), 2017 IEEE International Con-\\nference on, pages 786–790. IEEE, 2017.\\n[14] Diederik P Kingma and Jimmy Ba. Adam:\\nA\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980, 2014.\\n[15] B. McFee, E.J. Humphrey, and J. Urbano. A plan for\\nsustainable MIR evaluation. In 17th International So-\\nciety for Music Information Retrieval Conference, IS-\\nMIR, 2016.\\n[16] George A Miller. WordNet:\\na lexical database for\\nenglish. Communications of the ACM, 38(11):39–41,\\n1995.\\n[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\\nnay. Scikit-learn: Machine learning in Python. Journal\\nof Machine Learning Research, 12:2825–2830, 2011.\\n[18] Karen Simonyan and Andrew Zisserman. Very deep\\nconvolutional networks for large-scale image recogni-\\ntion. In International Conference on Learning Repre-\\nsentations, 2016.\\n[19] John Thickstun, Zaid Harchaoui, and Sham Kakade.\\nLearning features of music from scratch. In Interna-\\ntional Conference on Learning Representations, 2017.\\n444\\nProceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63ce15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
